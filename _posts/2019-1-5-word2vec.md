---
layout: post
title: word2vec
---
단어의 의미를 다차원 공간에 ‘벡터화’ 시킴으로 단어 사이에 유사도(의미론적)를 구할수 있다. 벡터화 된다는 것은 벡터연산을 통해 의미자체를 연산 할수 있다.
단어를 벡터화 하는 방법은 이미 다양한 연구가 있었고 그중에 word2vec 에 대해 이미 아는듯 하지만 잘 알지 못하는 것을 다시 잘 알아보자



# 개념

단어의 의미를 다차원 공간에 ‘벡터화’ 시킴으로 단어 사이에 유사도(의미론적)를 구할수 있다. 벡터화 된다는 것은 벡터연산을 통해 의미자체를 연산 할수 있다.

단어를 벡터화 하는 방법은 이미 다양한 연구가 있었고 그중에 word2vec 에 대해 이미 아는듯 하지만 잘 알지 못하는 것을 다시 잘 알아보자

왜 word embedding

문자로 이루어진 단어를 특정차원의 벡토로 변환하는 단계. 같은 문자에 대해 벡터로 변환 할수 있다. word embedding이 필요한 이유는 학습이나 알고리즘등의 처리를 위해서는 모두 숫자로 변환 되어야 한다.

**word embedding** 방법은 크게 두가지로 분류 할수 있다.

1. Frequency based Embedding - BOW , Count Vector , TF-IDF Vector
2. Prediction based Vector - CBOW, Skip-gram, CNN, RNN …

이하 우리가 알고자 하는 word2vec은 2013년 구글에서 발표된 모델로 CBOW와 SKip-gram을 구현한 라이브러리 Prediction based Vector 방법중에 하나이다.

## CBOW 모델

컨텍스트(주변단어)로부터 찾고자 하는 target를 predict 하는 모델이다.

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bad42f0a-ad54-4eb5-8369-54ed71ba0cb6/_2021-04-09__1.13.17.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bad42f0a-ad54-4eb5-8369-54ed71ba0cb6/_2021-04-09__1.13.17.png)

**N** - context count , **V** - 사전 크기

학습에 필요한 matrix는 2개 이다.

- Input Layer -> Projection Layer (Hidden Layer) : input layer의 모든 단어들이 **공통적**으로 사용하는 V*N matrix
- Projection Layer -> Output Layer : projection된 각 단어 벡터들의 평균을 구해서 Weight Matrix를 곱한다

## Skip-Gram모델

단어로부터 컨텍스트를 예측하는 방법

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9d3d8094-22ac-46c6-94d5-ea16f6d5e827/_2021-04-09__1.15.04.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9d3d8094-22ac-46c6-94d5-ea16f6d5e827/_2021-04-09__1.15.04.png)

## Complexity Reduction

Notion을 사용하면 폴더를 사용하지 않고도 페이지 내에 페이지를 중첩할 수 있습니다.

Output Layer에서 구현되는 SoftMax는 전체 사전의 단어를 계산해야 하기때문에 연산 비용이 높다. 이 연산을 줄이기 위한 방법이 Hierarchical softmax, negative sampling 등이 있다.

**Hierarchial SoftMax** multinomial distribution functiond을 사용해서 binary tree(ex Binary Huffman Tree)를 만들어서 단어의 확률를 계산할때 root -> leaf로 탐색하면서 확률를 곱해주는 방식

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8eebc2b6-b7cf-46ea-ad23-a870841a2e82/_2021-04-09__1.15.52.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8eebc2b6-b7cf-46ea-ad23-a870841a2e82/_2021-04-09__1.15.52.png)

hierarchial softmax를 사용하게 되면 Output Layer에서 W’matrix를 사용하지 않고 N(컨텍스트 길이)의 weight vector를 update 한다.

Negative Sampling

사전 전체를 softmax 하지 않고 sampling한 단어에 대해서 softmax연산을 한다. NxV -> NxK(sample count) positive sample(target) + negative sample

negative sampling방법은 실험적으로 정한다.
