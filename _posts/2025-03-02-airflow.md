---
layout: post
title:  Reddit Dynamic Product Ads 리뷰  
---

# Airflow 2.7 사용하기 


## airflow 내부 아키텍쳐
<img src="/assets/airflow/airflow1.png" width="60%" height="90%" style="display: block; margin: 0 auto;" />

## DAG의 구성요소
### 컴포넌트별 역활
#### Workder
역할: 태스크(Task)를 실제로 실행하는 컴포넌트
* CeleryExecutor나 KubernetesExecutor 환경에서 활성화됩니다.
* Airflow DAG 내의 개별 태스크 단위 작업을 받아 실행합니다.
* Python, Bash, SQL 등 다양한 Operator를 통해 실제 연산을 수행합니다.
* 실행 결과(XCom), 로그 등을 Metadata DB에 저장합니다.
* 리소스 관리가 중요: CPU, 메모리 설정을 태스크 유형에 맞게 조정 필요.

#### Scheduler
역할: DAG 실행 스케줄링과 태스크 상태 전환 담당
* DAG 파일을 주기적으로 파싱하여 DAG 구조 및 정의된 스케줄을 분석합니다.
* 실행 조건이 충족된 DAG Run 및 Task Instance를 생성합니다.
* 상태가 scheduled인 태스크를 실행 가능한 상태(queued)로 전환합니다.
* 그 외에도 trigger rule 평가, SLAs 감지, Dataset 기반 실행 조건 확인 등 많은 내부 로직 수행

#### Webserver
역할: 사용자 인터페이스(UI) 제공
* DAG 등록 여부, 상태, 실행 이력 등을 브라우저 기반 웹 UI로 시각화
* DAG 수동 실행, 재시도, 중단, 로그 보기 등의 운영 기능 제공
* dag_id, task_id, 실행 시간, XCom 값 등 다양한 정보를 조회/조작 가능
* Flask 기반으로 작동하며, 설정에 따라 인증/권한 체계(OAuth, LDAP 등) 구성 가능

#### Metadata DB
역할: Airflow의 중앙 저장소로서 모든 상태 정보를 저장
* DAG 정의, DAG Run, Task Instance, 로그 위치, XCom 값 등 모든 메타데이터를 저장
* Scheduler, Webserver, Worker가 이 DB를 통해 서로 상태를 공유하고 협업
* Airflow의 "싱글 소스 오브 트루스(single source of truth)"
* PostgreSQL, MySQL 등 관계형 DB를 사용하며, 동시성과 안정성이 중요

#### DAG directory
역할: DAG 파일을 저장하는 경로 (Python 스크립트 형식)
* .py 파일 형태의 DAG 정의 스크립트들을 배치
* Scheduler와 Webserver는 이 디렉토리를 주기적으로 파싱하여 DAG을 인식
* 로컬 디렉토리일 수도 있고, NFS, S3 마운트 등 외부 공유 스토리지일 수도 있음



## DAG 운용 베스트 프랙티스 (작성 및 모니터링)
효율적이고 견고한 DAG 운용을 위해서는 몇 가지 베스트 프랙티스를 따르는 것이 좋습니다. 아래에 Airflow DAG을 설계하고 운영할 때 유용한 지침들을 정리합니다.
### 커스텀 Operator의 구현 및 활용
Airflow 제공 Operator로 충분하지 않은 복잡한 작업은 사용자 정의 Operator를 만들어 재사용하는 것이 권장됩니다. Airflow 2.x 버전부터는 이러한 커스텀 Operator/Hook 코드를 플러그인 폴더에 직접 넣기보다 별도의 Python 패키지로 제작하여 관리하는 방식이 권장되고 있습니다  프로젝트별로 DAG 코드와 Operator 코드를 분리해두면 코드의 모듈화와 테스트가 수월해지고, 패키지로 배포하여 여러 환경(Airflow 인스턴스)에 쉽게 설치할 수 있습니다. 예를 들어 사내 PyPI 리포지토리에 커스텀 패키지를 업로드해 Airflow 워커에 설치함으로써, DAG에서는 일반 패키지 불러쓰듯 커스텀 Operator를 임포트해 사용하는 식입니다. 또한 커스텀 Operator 개발 시에는 Operator 간 의존성 충돌에 유의해야 합니다. 가능하면 Airflow가 제공하는 Hook나 미리 설치된 라이브러리를 활용하고, 추가 패키지가 필요할 경우 요구 사항을 잘 관리하여 Airflow 환경의 다른 패키지와 버전 충돌이 없도록 합니다. Airflow에서는 가상환경에서 연산을 실행해 의존성을 격리하는 PythonVirtualenvOperator나 외부 Python 인터프리터를 사용하는 ExternalPythonOperator 등을 지원하니, 복잡한 의존성이 필요한 경우 이런 Operator를 활용하는 것도 고려하세요

### 태스크의 오류 처리와 재시도 설계
DAG 내 개별 태스크는 오류 발생 상황을 견딜 수 있도록 설계하는 것이 중요합니다. Airflow는 태스크 재시도(retry) 메커니즘을 기본 제공하므로, 태스크 코드는 **멱등성(idempotency)**을 유지하여 재실행 시에도 동일한 결과를 내도록 해야 합니다. 예를 들어 DB 작업에서 단순 INSERT를 사용하면 재시도 시 중복 삽입이 발생할 수 있으므로 INSERT 대신 UPSERT를 사용하고, 입력 데이터도 항상 동일한 스냅샷(예: 특정 파티션의 파일/테이블)을 읽도록 하여 실행 시점에 따라 결과가 달라지지 않게 합니다 이처럼 태스크 결과가 한 번 성공하면 언제 재실행돼도 문제가 없도록 함으로써, Airflow의 자동 재시도 기능을 활용할 수 있습니다
```
## dag retry 설정 
@dag(default_args={
    'retries': 3,
    'retry_delay': datetime.timedelta(minutes=3),
    'on_retry_callback': task_retry_alert,
}
```
태스크에서 예외 처리를 할 때는 Airflow의 동작을 이해해야 합니다. 일반적으로 태스크 함수 내에서 발생한 예외는 Airflow가 받아서 태스크를 실패로 처리하고 재시도를 수행합니다. 만약 개발자가 try/except로 예외를 잡아버리고 넘어가면 Airflow는 태스크를 성공으로 인식하므로, 실패를 은폐하지 않도록 주의합니다. 
대신, 특정 예외 상황에서 태스크를 의도적으로 실패 또는 건너뛰기 처리하고 싶다면 Airflow가 제공하는 AirflowFailException이나 AirflowSkipException을 발생시켜 제어할 수 있습니다. 이러한 특별 예외를 사용하면 Airflow가 해당 태스크를 실패/스킵 처리하면서도 DAG의 흐름(예: 다음 태스크의 트리거 규칙)에 맞게 움직입니다. 

에러 모니터링과 알림도 중요한 부분입니다. Airflow는 태스크 성공/실패/재시도/SLA 미준수 등의 이벤트 시 자동으로 호출되는 콜백 함수나 알림 설정을 지원합니다. 예를 들어 DAG 정의 시 on_failure_callback에 함수를 등록하여 DAG 전체 실패 시 커스텀 동작을 실행하거나, 태스크 단위로 on_failure_callback을 지정해 특정 태스크 실패 시 알림을 보내는 식으로 활용할 수 있습니다.
```
#  dag 실패 알림 함수 등록 
@dag(default_args,
    on_failure_callback=dag_failure_alert,
)

# task 실패 알림 함수 등록
@task.python(on_failure_callback=task_failure_alert)


# sentry 알림 설정 
def dag_failure_alert(context):
  """
  dag 이 수행 실패 했을때, sentry 에 기록한다.

  :param context:
  :return:
  """
  sentry.set_dagname(context['dag'].dag_id)

  err = f"""
        !! AIRFLOW EXCEPTION!!
        dag: {context['dag'].dag_id}
        execution_date: {context['execution_date']}
        exception: {context['exception']}
    """
  sentry.error(err)
```

기본 설정으로는 이메일 알림이 가능하며, Slack이나 PagerDuty와 같은 외부 시스템과 연계하는 경우 SlackWebhookOperator나 전용 Notifier 클래스를 활용할 수 있습니다
다만 SLA 관련 알림은 스케줄링된 DAG에만 적용되는 제한이 있습니다
수동으로 트리거한 DAG 실행의 SLA miss에는 기본 알림이 동작하지 않으니, 이런 경우엔 별도의 모니터링을 구현해야 합니다. 또한 DAG 차원에서 Fail Stop 모드(fail_stop=True)를 사용하면 앞서 소개한 것처럼 첫 실패 발생 시 나머지 태스크를 모두 중단할 수 있습니다
이를 통해 불필요한 리소스 소모를 막고 조기에 실패를 감지할 수 있지만, 이 경우 모든 태스크의 트리거 규칙을 기본값(all_success)으로 유지해야 한다는 점을 기억해야 합니다

### DAG 태그(Tag) 및 SLA 설정 활용
태그(Tag)를 활용하면 많은 DAG을 운영할 때 손쉽게 DAG들을 분류하고 관리할 수 있습니다. DAG 객체를 생성할 때 tags=['팀명', '프로젝트명']과 같이 태그를 지정해두면, Airflow UI에서 태그별로 DAG 필터링을 할 수 있어 관련 DAG만 모아서 보기가 편해집니다. 
예를 들어 대시보드에서 "finance" 태그로 필터링하면 금융 관련 DAG들만 목록에 표시할 수 있습니다. 태그는 DAG 파일 파싱 시 메타데이터로 등록되며, 태그가 더 이상 필요없게 된 경우 airflow db clean 명령으로 제거할 수도 있습니다
일관된 태그 체계를 마련해 두면 대규모 환경에서 DAG를 팀별/기능별로 그룹화하여 파악하기에 유용합니다.

SLA(Service Level Agreement)는 태스크가 정해진 시간 내 완료되지 못했을 때를 감지하는 기능입니다. 
각 태스크에 sla=datetime.timedelta(hours=1)처럼 SLA를 지정하면, 해당 태스크가 실행 시작 시각으로부터 1시간 내 완료되지 않을 경우 SLA miss로 기록됩니다. SLA miss 발생 시 Airflow는 기본적으로 특별 알림 이메일(환경에서 설정된 smtp 옵션 사용)을 보내도록 되어 있어, 태스크 지연을 운영자가 인지할 수 있게 합니다. 그러나 앞서 언급했듯이 SLA 알림은 스케줄 실행에만 적용되므로, 수동 또는 외부 트리거 DAG에는 동작하지 않는 점에 유의해야 합니다. 
SLA 미준수 상황은 Airflow UI의 “SLA Misses” 화면이나 로그를 통해서도 확인 가능합니다. 다만 최근에는 SLA 대신 태스크별 execution_timeout을 걸고 실패시 자체 알림을 구현하거나, DAG 실패 콜백에서 전체 실행 지연을 체크하는 방식 등으로 대체하는 경우도 많습니다. 
환경에 맞게 SLA 기능을 활용하되, 필요하다면 외부 모니터링과 조합하여 파이프라인 지연 상황을 빠르게 포착할 수 있는 체계를 갖추는 것이 좋습니다.
SLA설정의 알림 발송은 태스크 종료 이후 다음 task 실행 시점에 발송되는 것을 주의 해야 합니다. 
```
# DAG SLA 설정
@dag(default_args={
    'sla': timedelta(minutes=15)  
}
```

### DAG 문서화
Airflow의 DAG 정의 시 사용하는 매개변수 중 하나로, DAG의 설명을 마크다운 형식으로 UI에 표시할 수 있도록 해주는 기능입니다. 
해당 모듈 혹은 파일 최상단에 있는 DocString을 UI에 표시해 줍니다. 이를 통해 문서를 코드내에 위치 시키고 UI에서 실행시 가이드를 제공 할수 있습니다. 
```
@dag(default_args,
     doc_md=__doc__,
)
```
### Operator 
PythonOperator
```
from airflow.operators.python import PythonOperator
def my_func():
    print("Hello from Python!")

task = PythonOperator(
    task_id='run_python',
    python_callable=my_func,
    dag=dag,
)
```
BashOperator
```
from airflow.operators.bash import BashOperator

task = BashOperator(
    task_id='run_bash',
    bash_command='echo "Hello from bash"',
    dag=dag,
)
```
DummyOperator
```
from airflow.operators.empty import EmptyOperator

start = EmptyOperator(task_id='start')
```
CustomOperator
``` 
from airflow.models import BaseOperator

class MyCustomOperator(BaseOperator):
    def execute(self, context):
        # 실제 작업
        print("Doing my custom job")
``` 

### Sensor 
Airflow에서 **Sensor(센서)**는 일정 조건이 만족될 때까지 기다리는 역할을 수행하는 특별한 Operator입니다. 워크플로우 내에서 외부 이벤트나 리소스 상태를 감지할 필요가 있을 때 사용됩니다.

FileSensor(특정 경로에 파일이 생성될 때까지 대기)
```
from airflow.sensors.filesystem import FileSensor

wait_for_file = FileSensor(
    task_id='wait_for_data_file',
    filepath='/data/input.csv',
    poke_interval=30,  # 조건을 확인하는 간격 (초)
    timeout=3600,   # 기다릴 수 있는 최대 시간 
    mode='reschedule',    # poke:대기시간 동안 워커를 점유,  reschedule: 워커 반환 후 다시 스케쥴링 
)
```

ExternalTaskSensor(다른 DAG의 특정 태스크가 완료될 때까지 대기)
```
from airflow.sensors.external_task import ExternalTaskSensor

wait_for_task = ExternalTaskSensor(
    task_id='wait_for_upstream',
    external_dag_id='upstream_dag',
    external_task_id='finalize_task',
    allowed_states=['success'],
    timeout=600,
)
```
Airflow 2.4 이상에서는 Dataset 기능을 활용해 센서를 대체할 수 있습니다. 예를 들어 DAG A에서 파일을 생성하고, DAG B가 그 파일을 감지하는 구조라면:

### DataSet
Dataset은 “데이터의 위치 또는 식별자”를 나타내는 객체로, 특정 DAG(Task)가 어떤 데이터를 생산하거나 소비하는지를 나타냅니다.
data를 중심으로 Dag의 실행 트리거르 구성함으로 타입 스케쥴의 한계를 극복하고 센서를 통한 리소스 낭비를 줄일 수 있습니다. 

```
# dataset 객체 정의 
from airflow.datasets import Dataset
my_dataset = Dataset("s3://my-bucket/sales.csv")
```

```
# 생산자 
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def generate_csv():
    # 데이터 파일 생성 로직
    ...

with DAG(
    dag_id="generate_sales_csv",
    start_date=datetime(2024, 1, 1),
    schedule_interval="@daily",
    catchup=False,
) as dag:
    create = PythonOperator(
        task_id="create_csv",
        python_callable=generate_csv,
        outlets=[my_dataset],  # ← Dataset 생산
    )
```
소비자
```
with DAG(
    dag_id="process_sales_csv",
    start_date=datetime(2024, 1, 1),
    schedule=[my_dataset],  # ← Dataset 소비
    catchup=False,
) as dag:
    process = PythonOperator(
        task_id="process_csv",
        python_callable=process_file,
    )
```




