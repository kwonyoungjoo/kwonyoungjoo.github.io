I"U<p>모든 시스템은 현재와 개선된 모델의 비교 평가가 매우 중요하다. 절대적인 수치는 표현하기 어렵지만 좀 더 개선된 것에 대한 지표가 있어야 우리가 나아갈 방향 혹은 논리적인 확신을 가지고 개발을 할수 있다.</p>

<p>Text Summarization 시스템을 만들고 개선해 가려면 평가지표가 있어야 하는데 요약관련 논문에서 많이 사용하던 지표 ROUGE를 알아보자</p>

<p>System Summary</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>the cat was found under the bedSystem Summary
</code></pre></div></div>

<p>System Summary</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>the cat was under the bed
</code></pre></div></div>

<p>모델의 결과물과 정답이 얼마나 유사한지를 측정하기 위해서는 precision과 recall를 종합적으로 분석해 보아야 한다.</p>

<p>정답이 모델과 얼마나 유사한가를 나타내는 지표. 정답이 짧거나 모델의 예측결과가 정답을 모두 커버하는 경우 recall이 높게 나올수 있다. 하지만 예측결과가 정답을 포함하고 무한대로 길어지는 경우도 recall은 높게 나온다. 이 점을 보완하기 위해서 precision을 평가에 참여 시켜야 한다.</p>

\[recall = commonwordCount/referenceSummarywordcount\]

<p>모델이 정답과 얼마나 유사한가를 나타내는 지표</p>

\[precision = commonwordcount/systemSummarywordCount\]

\[f - measure = 2x* (precision * reall ) / (precision + recall)\]

<p>ROUGE-N – measures unigram, bigram, trigram and higher order n-gram overlap</p>

<p>ROUGE-L – measures longest matching sequence of words using LCS. An advantage of using LCS is that it does not require consecutive matches but in-sequence matches that reflect sentence level word order. Since it automatically includes longest in-sequence common n-grams, you don’t need a predefined n-gram length.</p>

<p>ROUGE-S – Is any pair of word in a sentence in order, allowing for arbitrary gaps. This can also be called skip-gram coocurrence. For example, skip-bigram measures the overlap of word pairs that can have a maximum of two gaps in between words. As an example, for the phrase “cat in the hat” the skip-bigrams would be “cat in, cat the, cat hat, in the, in hat, the hat”.</p>

<h3 id="요약평가">요약평가</h3>

<p>extractive한 요약에서는 rouge평가를 사용해서 모델을 개선하는 방향으로 했는데 abstractive 요약모델은 여전히 평가가 어렵다. token을 generation하기 떄문에 동일한 단어를 가진다는 의미가 무의미하기 떄문이다. 그렇다면 word2vec의 값을 이용해서 최소값을 찾아야 할까? abstractive 요약의 정량적 평가는너무 어렵다.</p>

<h3 id="문제발생">문제발생</h3>

<p>bert를 이용한 한국어 요약 모델에서 Recall은 높은데 Precision이 낮은 경우가 발생했다.</p>

<p>둘의 관계가 trade off임을 감안하더라도 너무 큰 격차가 발생했고 그 결과 F-Score(recall과 precision의 조화평균)값도 낮게 측정되었다. 첫번째 가능성은 요약되어 나온 본문에 비해 label로 사용한 제목의 길이가 매우 짧았다. label을 다른 데이터로 바꿔야 할것 같음</p>
:ET